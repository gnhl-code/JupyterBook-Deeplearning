{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a9ad4d",
   "metadata": {},
   "source": [
    "# Riding the Learning Curve: How a Single Number Decides Whether Your Neural Network Succeeds or Crashes\n",
    "\n",
    "Date: November 19, 2025         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461f6d8",
   "metadata": {},
   "source": [
    "Training a deep learning model is a lot like teaching a student.\n",
    "If you teach too slowly, the student never learns enough.\n",
    "If you teach too quickly, the student gets confused and makes wild mistakes.\n",
    "\n",
    "In neural networks, this teaching speed is controlled by the learning rate — one simple number that can decide whether your model becomes smart… or completely fails.\n",
    "\n",
    "This blog explores what a learning rate is, why it is so powerful, and how changing it affects training in real-life experiments. Using a simple CNN on the MNIST dataset, we’ll compare learning rates and visualize how training behaves at different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1a075",
   "metadata": {},
   "source": [
    "## What Is Learning Rate?\n",
    "\n",
    "Learning Rate is a key hyperparameter in neural networks that controls how quickly the model learns during training. It determines the size of the steps taken to minimize the loss function. It controls how much change is made in response to the error encountered, each time the model weights are updated. It determines the size of the steps taken towards a minimum of the loss function during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4975a",
   "metadata": {},
   "source": [
    "**_In short, its a hyperparameter that controls how much the model updates its weights in response to the error it makes._**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319423b",
   "metadata": {},
   "source": [
    "Formally, it appears in the gradient descent update rule:\n",
    "\n",
    "$$θt+1​=θt​−η⋅∇θ​J(θ)$$\n",
    "\n",
    "\n",
    "where: \n",
    "- $θt$ = current parameters\n",
    "- $θt+1$ = updated parameters\n",
    "- $∇θ​J(θ)$ = gradrient of the loss functions\n",
    "- $η(eta)$ = learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18354437",
   "metadata": {},
   "source": [
    "## Why is this important?\n",
    "\n",
    "_Because the learning rate determines how big each update step is._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e101b51",
   "metadata": {},
   "source": [
    "| Learning Rate | What Happens                                    |\n",
    "| ------------- | ----------------------------------------------- |\n",
    "| **Too Low**   | Model learns extremely slowly, may get stuck    |\n",
    "| **Ideal**     | Smooth, stable learning and fast convergence    |\n",
    "| **Too High**  | Model becomes unstable → oscillates or diverges |\n",
    "\n",
    "**Visual analogy:** <br>\n",
    "Small LR → baby steps <br>\n",
    "Medium LR → walking normally<br>\n",
    "Large LR → running downhill<br>\n",
    "Very large LR → falling off a cliff<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9e404",
   "metadata": {},
   "source": [
    "## Why Learning Rate Matters\n",
    "\n",
    "1. Controls how fast the model learns\n",
    "A higher learning rate speeds up learning at the cost of stability.\n",
    "2. Affects the quality of the final solution\n",
    "A bad learning rate can trap the model in poor local minima or saddle points.\n",
    "3. Determines training stability <br>\n",
    "Too high → training “explodes”. <br>\n",
    "Too low → training drags for hours.\n",
    "\n",
    "4. Interacts with optimizers\n",
    "For optimizers like Adam, RMSProp, and SGD, the default LR is often set to values that balance speed vs. reliability. But there’s never a perfect LR for every model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f30fd",
   "metadata": {},
   "source": [
    "## To understand learning rates in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e2053",
   "metadata": {},
   "source": [
    "**To understand learning rates in action, we trained a simple Convolutional Neural Network (CNN) on the MNIST handwritten digit dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3be292",
   "metadata": {},
   "source": [
    "### Experiment Setup\n",
    "\n",
    "- Dataset: MNIST\n",
    "- Model: Simple CNN\n",
    "- Epochs: 5\n",
    "- Optimizer: Adam\n",
    "- Learning Rates Tested:\n",
    "    - **0.0001 (very low)**\n",
    "    - **0.001 (recommended for Adam)**\n",
    "    - **0.01 (high)**\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Show how different learning rates affect:\n",
    "- Training loss\n",
    "- Training speed\n",
    "- Final test accuracy\n",
    "- Training stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72849989",
   "metadata": {},
   "source": [
    "1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906ecc3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\__init__.py:257\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m is_loaded = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     res = \u001b[43mkernel32\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m     last_error = ctypes.get_last_error()\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error != \u001b[32m126\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245f2ce",
   "metadata": {},
   "source": [
    "2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f5497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351d3af",
   "metadata": {},
   "source": [
    "3. Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d97f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.fc(self.conv(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715d861",
   "metadata": {},
   "source": [
    "4. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7106f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(lr):\n",
    "    model = CNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        for x, y in train_loader:\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        print(f\"LR={lr}, Epoch={epoch+1}, Loss={loss.item():.4f}\")\n",
    "    return model, loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a49924",
   "metadata": {},
   "source": [
    "5. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model, losses = train_model(lr)\n",
    "    results[lr] = losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7e984",
   "metadata": {},
   "source": [
    "6. Plot Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acef2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr, loss in results.items():\n",
    "    plt.plot(loss, label=f\"LR={lr}\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Comparison Across Learning Rates\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bdb35",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Hyperparameter Sensitivity: The learning rate is a decisive factor in model training. Even with identical architectures, variations in LR can determine the difference between rapid convergence and model divergence.\n",
    "\n",
    "The \"Goldilocks\" Principle:\n",
    "- Too Low: Can lead to slow training or, as seen here, getting stuck in suboptimal states.\n",
    "- Too High: Causes oscillation and inability to reach the absolute global minimum.\n",
    "- Optimal: Facilitates smooth, efficient descent (e.g., $\\alpha = 0.001$ in this experiment).\n",
    "\n",
    "Visual Diagnostics: Plotting loss curves is essential. Numerical accuracy metrics alone often hide how the model is learning (or failing to learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ff96a",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "References\n",
    "Academic Sources\n",
    "\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press. http://www.deeplearningbook.org\n",
    "\n",
    "Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR). https://arxiv.org/abs/1412.6980\n",
    "\n",
    "Technical Documentation & Tutorials\n",
    "\n",
    "PyTorch. (2024). Optimizers - PyTorch Documentation. Retrieved from https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "Li, F., Karpathy, A., & Johnson, J. (n.d.). CS231n: Convolutional Neural Networks for Visual Recognition (Optimization). Stanford University. https://cs231n.github.io/optimization-1/\n",
    "\n",
    "Brownlee, J. (2022). A Gentle Introduction to Learning Rate in Deep Learning. Machine Learning Mastery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa375a",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "The learning rate is often the single most significant hyperparameter to tune in deep learning. As shown in the visual analysis, a magnitude change in learning rate (e.g., from $10^{-4}$ to $10^{-3}$) dramatically alters the training trajectory. For future iterations, utilizing Learning Rate Schedulers (decaying the rate over time) is recommended to combine the speed of high initial rates with the precision of low final rates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}