{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12fc715-b64e-40db-bd69-7b7ee993c205",
   "metadata": {},
   "source": [
    "# Laboratory Task 2\n",
    "\n",
    "Genheylou Felisilda - DS4A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d14106-932c-490f-b858-f8c893b2b5ed",
   "metadata": {},
   "source": [
    "Instruction: Perform a single forward pass and compute for the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90fb11-cdbf-4cfc-9513-bb667c356b99",
   "metadata": {},
   "source": [
    "$\n",
    "x = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "y = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "f(z) = max(0, Z_n)\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{hidden unit weights} =\n",
    "\\begin{bmatrix}\n",
    "w_{11} & = & 0.2   & w_{12} & = & -0.3 \\\\\n",
    "w_{13} & = & 0.4   & w_{14} & = & 0.1  \\\\\n",
    "w_{15} & = & -0.5  & w_{16} & = & 0.2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{output unit weights} =\n",
    "\\begin{bmatrix}\n",
    "w_{21} & = & -0.3 \\\\\n",
    "w_{22} & = & -0.2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_1 & = & -0.4 \\\\\n",
    "\\theta_2 & = & 0.2 \\\\\n",
    "\\theta_3 & = & 0.1\n",
    "\\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488f9eb-fbd9-4135-b85e-33c971882d8a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"font-family: Arial\">\n",
    "<p style=\"font-family:Times New Roman; text-align:justify; font-size:15px\">\n",
    "\n",
    "### **Solution**\n",
    "\n",
    "**Hidden Layer**\n",
    "\n",
    "Compute each hidden unit pre-activation:\n",
    "\n",
    "$$\n",
    "z_1 = (1)(0.2) + (0)(0.4) + (1)(-0.5) + (-0.4) = -0.7\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_2 = (1)(-0.3) + (0)(0.1) + (1)(0.2) + (0.2) = 0.1\n",
    "$$\n",
    "\n",
    "Apply activation $(a_i = f(z_i))$:\n",
    "\n",
    "$$\n",
    "a_1 = f(z_1) = f(-0.7) = 0, \\quad\n",
    "a_2 = f(z_2) = f(0.1) = 0.1\n",
    "$$\n",
    "\n",
    "So the hidden activations are:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Output Layer**\n",
    "\n",
    "Weights and bias:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(o)} =\n",
    "\\begin{bmatrix}\n",
    "-0.3 \\\\\n",
    "-0.2\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\theta^{(o)} = 0.1\n",
    "$$\n",
    "\n",
    "Pre-activation:\n",
    "\n",
    "$$\n",
    "z_o = (a_1)(-0.3) + (a_2)(-0.2) + \\theta^{(o)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_o = (0)(-0.3) + (0.1)(-0.2) + 0.1 = 0.08\n",
    "$$\n",
    "\n",
    "Activation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(z_o) = f(0.08) = 0.08\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Error Calculation**\n",
    "\n",
    "Target:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix} 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Error:\n",
    "\n",
    "$$\n",
    "E = \\tfrac{1}{2}(y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "E = \\tfrac{1}{2}(1 - 0.08)^2 = \\tfrac{1}{2}(0.92^2) = 0.4232\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Final Results:**\n",
    "\n",
    "$$\n",
    "\\hat{y} = 0.08, \\quad E = 0.4232\n",
    "$$\n",
    "\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b813d-c23f-4abf-9b05-26be0790c232",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this solution, we used a simple neural network to process an input and make a prediction. We passed the input through a hidden layer using ReLU as the activation function, which turned negative values into zero. After that, the result went through the output layer to get the final prediction, which was 0.08. We then compared it to the actual target value of 1.0 and calculated the error as 0.4232. This shows that the network made a prediction, but it still needs improvement. To reduce the error, we would need to continue training the network using backpropagation and update the weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}